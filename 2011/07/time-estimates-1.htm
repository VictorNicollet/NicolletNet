@title The Art of Development Time Estimates (Part 1)
@draft

<img class="aligncenter size-full wp-image-2437" title="tree" src="http://www.nicollet.net/wp-content/uploads/2011/07/tree.png" alt="" width="675" height="100" />

Writing software takes time, and time is money both in terms of programmer wages and in terms of delayed releases. It makes sense to try and predict ahead of time how long a given feature would take, in order to make an informed decision about whether it should be attempted, reduced or eliminated. If your job is to predict durations, make sure you understand whether you are expected to provide a back-of-the-envelope <em>approximation</em> — with the implication that it could be wrong by an order of magnitude in both directions — or if you're going for a  <em>guarantee</em> — this feature will cost no more than X days of work, unless something really catastrophic occurs, which is what a paying customer wants to know.

If your co-workers ever start using your approximations to define milestones, prepare deadlines and discuss delays, you have not insisted enough on the fact that it was an approximated answer. If anyone asks me for an on-the-fly estimate, I provide an upper and lower bound as "this will take somewhere between 2 and 10 days". This is an outrageously wide range, but it's fairly correct in terms of how wrong I can be with my on-the-fly estimates, and it deters anyone from just adding up the estimates to come up with a deadline. Yes, some people have tried converting "between 2 and 10 days" to "around 5 days" but I gave them the evil eye every single time. If anyone needs to turn an approximation into a guarantee, there is no sane reason to use anything but the upper bound.
<h3>What Could Go Wrong ?</h3>
There's a fairly common mistake to be made with the upper bounds, and I've made it myself quite a lot: not being pessimistic enough. We're lazy humans, so being optimistic is natural: we come up with a few tasks that need to be done, slap a reasonable duration on each one, and add them up. The upper bound is then pulled out of a top hat as being two to three times higher than the lower bound, because <em>that feels right</em>. Quite to the contrary, the upper bound must be calculated by actively looking for those things that can go wrong. Newbie programmers can usually provide a fairly accurate optimistic estimate because knowing <em>what needs to be done</em> is a prerequisite of being a programmer at all, but the pessimistic estimate requires knowledge of <em>what can go wrong</em>, which by definition is an esoteric list of accidents gathered from experience rather than rational forethought:
<ul>
	<li>The feature involves changing some code that is unusually brittle or unstable, so time will be needed to either pay the technical debt up front and bring that code back to acceptable quality levels, or soak up the cost of hunting for bugs after the code has been changed. This is the most frequent issue I encounter when dealing with changes to existing software, because not all code is of equal quality regardless of how much effort you put into it.</li>
	<li>A library does exist, but preliminary analysis failed to observe that it only supports 95% of the required feature set, so additional time is necessary to obtain the missing 5%. Several internet-facing modules in one of my recent projects use a standard library for doing HTTP requests, but I discovered late during development that said library did not support HTTPS, which prompted me to include a second library, and incur technical debt related to having two overlapping libraries in the same project.</li>
	<li>The library does fulfill all requirements, but happens to contain an obscure bug that prevents the feature from working as expected, so more time is spent trying to work around the bug and get the library authors to fix it. This is especially nasty when no replacement is possible, such as <a href="http://stackoverflow.com/questions/6549648/strange-error-message" target="_blank">errors in database servers</a>.</li>
	<li>The code works as written, but QA testing reveals massive performance issues on typical user input, and time is required to correct the issue. On an older project, I used a <a href="http://www.fyneworks.com/jquery/star-rating/" target="_blank">jQuery plugin</a> for handling five-star ratings, with a single rating component costing 300 milliseconds in initialization — nothing noticeable on our test pages where only one component was used, but it brought the page load time to an unacceptable three seconds because users created feedback polls with dozens of such components.</li>
	<li>The programmer who implemented the first half of the feature is ill, on vacation, fired, fighting fires on another project, demotivated, stuck in the snow or otherwise unavailable. Another developer is brought in and needs to spend some time getting familiar with the half-completed code (and getting that uncommitted code from the unavailable developer's laptop was, in itself, a delay).</li>
	<li>The programmer who implemented the feature delivered an incomplete buggy product several days late.</li>
	<li>The programmer misunderstood the requirements and implemented the wrong feature.</li>
	<li>An unforeseen edge case is detected that has severe consequences on the application architecture. For instance, a given server-side process was assumed to be synchronous but is discovered to be asynchronous with latencies of several minutes on high server load. This makes the original plans for a five-second loading page obsolete, and calls for a costlier, asynchronous "we'll start working on this and notify you when we're done" user interface strategy instead.</li>
</ul>
The list goes on. Think of it as a shopping list you can go through when coming up with a pessimistic upper bound — start with the lower bound and add possible accidents.

Announcing a "between 2 and 10 days" range out of the blue can sound ridiculous, but it is quite less so when it's actually backed by a list potential problems. Eight days spent working around library issues, obscure edge cases and performance problems is actually pretty normal from my own experience if these problems <em>do</em> come up.

Stay tuned for the next issue, where I will discuss how to work with your team and your stakeholders to lower those estimates.

<small>Article image © Alexandre Pereira</small><small> — </small><small> <a href="http://www.flickr.com/photos/apr77/5927639523/">Flickr</a></small>