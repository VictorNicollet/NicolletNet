@title On NoSQL Design Tips
@draft

Cloud computing expert Joannes Vermorel posted <a href="http://vermorel.com/journal/2011/4/5/a-few-design-tips-for-your-nosql-app.html" target="_blank">a few design tips for your NoSQL app</a> two weeks ago. As a short anecdote, Joannes is the man who (through a rather unintended and surprising chain of events) prompted us to move from SQL to CouchDB six months ago on the RunOrg project. So, here are my very own thoughts overlayed on top of Joannes' five-point article :

<strong>You need an O/C (Object-to-Cloud) mapper</strong> : From my experience, almost all read-only code in the application involves some form of object graph traversal : select a root object, extract a list of related objects, filter the list based on a predicate... Most of the traditional Object-Relational impedance mismatch is a consequence of SQL's insistence on performing most of that traversal on the database server using joins in a conceptually parallel environment (every non-discarded row is going to be treated the same by the join) where object-oriented strategies dictate that each object should be able to react differently based on its type. So, SQL-and-Objects leaves you with a sprinkle of duplicate traversal logic across tiers, and some low-performance application-layer graph traversal where polymorphism requires it. And by low-performance, I mean two hundred one-item requests in a row :
<pre style="padding-left: 30px;">foreach (Document document in documents)
  owners.add(document.getOwner());</pre>
And there's no simple way of dealing with this, short of dictating that documents don't have owners, they have owner identifiers that can then be used to retrieve the actual owners from the database (which may be the case for persistent documents, but is a leaky abstraction that prevents the rest of your application from creating temporary in-memory documents). With a few exceptions, in NoSQL, there are no joins. On the one hand, this reduces the expressiveness of the data storage layer, but this also has the ironic consequence of making NoSQL more optimal for application-driven graph traversal. Adding insult to injury is that most key-value stores implement bulk queries which, when appropriately leveraged by the mapper, transparently improve performance. <strong>Breathe</strong> (the CouchDB mapper we use) does this :
<pre style="padding-left: 30px;">let owners = documents |&gt; Breathe.batch (fun doc -&gt; doc # get_owner)</pre>
Since <strong>Breathe</strong> is purely functional and based on monads, any requests performed by the many <code>doc # get_owner</code> calls can be merged into a single CouchDB request using the bulk API.

<img class="alignright size-full wp-image-2229" style="margin-left: 10px;" title="logo" src="http://www.nicollet.net/wp-content/uploads/2011/02/logo.png" alt="" width="175" height="150" />Another reason why mappers are necessary is that unlike SQL, which works on an extremely simple "send query, get results" basis, most NoSQL solutions involve complex idioms using basic primitives. For instance, when dealing with CouchDB, you cannot press the "use transactions" button and expect it to work. CouchDB transactions involve posting a document, receiving a collision notification, downloading the new version of the document, <em>applying the transformation again</em>, posting the document again, and so on until you either run out of retries or no collision happens. And collision detection involves keeping a revision number around and passing it to the API, so your code also has to deal with whether there's a cached in-memory copy of the object that you can use to grab the revision or if you will have to actually query the database. The public API provides nothing to relieve this pain, so it is up to the mapping system to do this :
<pre style="padding-left: 30px;">let publish id =
  let publish article = { article with published = true } in
  Article.DB.transaction id (Article.DB.update publish)</pre>
The mapper will grab the initial page value from the database if it was not already in the cache, try the update 20 times if collisions happen, and place the latest version of the page in the cache. Using raw API calls, this would take ten lines and probably contain many bugs. And did I mention that the function above can use the bulk transaction API if placed in a loop ?

<strong>Performance is obtained mostly by design</strong> : which I would rephrase as "your O/C mapper abstracts operations, not architecture". In short, there's no way for your application to shield itself from the architectural consequences of NoSQL storage. For instance, CouchDB just <em>doesn't</em> do multi-object transactions, so this is something that the application will have to take into account — this means that most operations are idempotent so they can be safely retried, and most cross-object invariants are expected to be <em>eventually</em> consistent.

And, indeed, you cannot write an application and then leave it to the database administrators to create an index wherever necessary. The good news is that most NoSQL solutions, by their simple design, make it hard to do any inefficient operations (it takes one line to do a full table scan in SQL, but it's a lot harder to do with NoSQL, and a lot more obvious when it happens — why am I querying <code>_all_docs</code> again?)

The 20 updates/second limit should be taken with a grain of salt. NoSQL performance is certainly predictable, but sweeping generalizations never apply and there are many bottlenecks that you can hit in any particular order — application-database bandwidth, database-disk bandwidth, database processor usage... right now, CouchDB handles 130 updates/second (an update is a GET-PUT cycle), with the bottleneck being that the updates are synchronous on the application side, so it doesn't send as many requests as the database could possibly handle (if you run two application instances, you can get 260 updates/second). This is a fairly healthy bottleneck to have, because it means adding more application instances solves it, and this just happens to be our scaling strategy ;-)

<strong>Go for a contract-based serializer</strong> : automatic serialization is great for one-shot messages that run between instances of the application, but a pain as soon as you need to persist them and, consequently, have the storage format evolve along with the application.

I have no opinion on the XML-vs-JSON debate, though I use JSON myself because it fits with both the database and client layers of my project.

In general, I find the use of NoSQL to be no excuse for making a mess in the database. Sure, you're not constrained by a schema anymore, but data in the database is what you are going to manipulate ! If there isn't a clean description of what you can find in the database, that data might as well be lost, because you will not be using it. Using application-wide conventions on data storage and representation helps developer A manipulate data generated by developer B without requiring a meeting to get everyone on the same page. Storing data in an opaque blob representation is fine for short-lived situations, obscure and rare edge cases (such as when the Lokad.Cloud queue offloads large objects into blob storage because it <em>has</em> to put them somewhere), but most of your data should have a clear documented schema. Your database does not need it, but your team does.

<strong>Entity isolation is easiest path to versioning </strong>:<strong> </strong>You Ain't Gonna Need I (yes, I disagree with Joannes on this one). Unless something went terribly wrong with your code base, it's  to duplicate your entity class when the two versioning paths actually start to diverge (and even then, you can certainly keep some common bits factored out nicely) instead of doing so from the very beginning.

<strong>With proper design, aka CQRS, needs for SQL drop to near-zero</strong> : Well, let's face it: SQL provides a toolset that goes beyond anything any NoSQL solution in terms of general flexibility and expressiveness (including the ability to simulate key-value stores using blobs,should the need arise). NoSQL solutions remain niche solutions to niche problems because this is the way they are designed, possibly out of a fear of inadequacy and the general message "NoSQL should be used where SQL is not good enough" — most NoSQL users still use an SQL database as the primary storage, and dump data into NoSQL for a small subset of queries that need the performance boost. Companies like Lokad or RunOrg, that embrace NoSQL-only architectures, are still rare <em>and still struggling to understand how, exactly, some things should be done in NoSQL that would take one line of SQL code</em>. You don't see <em>Design Tips for SQL</em> articles being written in 2011 ;-)

Even with CQRS, requirement changes have a much greater impact on NoSQL application architecture than they would with SQL, because SQL mappers despite all their flaws still manage to provide a significant amount of shielding. What used to be a matter of adding a few tables and doing the right joins now turns into an uphill battle of propagating the appropriate de-normalized data into the right places to allow for an index to be built (which, by the way, leads me to believe that Aspect-Oriented Programming might be more adapted for NoSQL than standard OOP) that simply isn't possible without a nearly extremist agile culture. Sure, with CQRS and event sourcing, this isn't a <em>losing</em> uphill battle we're fighting anymore, but it's still uphill. I'm pretty sure the reason why Joannes does not see things this way is that Lokad <em>has</em> an extremist agile culture.